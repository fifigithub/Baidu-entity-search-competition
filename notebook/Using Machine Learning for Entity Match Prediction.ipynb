{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many factors can help us decide whether a term matches with an entity, e.g. the number of word/character overlaps in names and baike definitions, type of query, word association etc. I plan to use a linear model to glue these factors together.\n",
    "\n",
    "In this framework, I build a logistic regression to assign a probability for whether an entity matches with a given term. Training data would be every query term and entity candiate pair, and the gold label; decoding would be to rank entities based on the matching probabilities.\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. How well can we predict in every category?\n",
    "2. What features would work well? Should we use other categories as part of the training data?\n",
    "3. What info seems to be missing?\n",
    "\n",
    "To answer these questions:\n",
    "\n",
    "1. I cross-validate on all data, using features either combined with category labels or not\n",
    "  1. number of word/characters with an overlap\n",
    "    1. In name\n",
    "    2. In baike\n",
    "  2. Word/characters that overlapped\n",
    "    1. In name\n",
    "    2. In baike\n",
    "2. I record cross-validate result into a report for error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Imports\n",
    "import sklearn\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from os import path\n",
    "import jinja2\n",
    "from IPython.display import display, HTML\n",
    "import unicodecsv as csv\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import linear_model\n",
    "from sklearn import cross_validation\n",
    "\n",
    "template_dir = path.abspath('../html')\n",
    "loader = jinja2.FileSystemLoader(template_dir)\n",
    "environment = jinja2.Environment(loader=loader)\n",
    "\n",
    "\n",
    "def apk(actual, predicted, k=None):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if k is None:\n",
    "        k = len(predicted)\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "sub_tasks = ['celebrity', 'movie', 'restaurant', 'tvShow']\n",
    "trainset_locs = map(path.abspath, ['../data/TRAIN SET/%s.TRAINSET.txt' % t for t in sub_tasks])\n",
    "devset_locs = map(path.abspath, ['../data/DEV SET/%s.DEVSET.txt' % t for t in sub_tasks])\n",
    "output_locs = map(path.abspath, ['../output/%s.txt' % t for t in sub_tasks])\n",
    "\n",
    "\n",
    "# loading datasets\n",
    "def LoadInData(data_loc, test_data=False):\n",
    "    lines = unicode(open(data_loc).read(), 'gbk').split('\\n')\n",
    "    parsing_result = []\n",
    "    for line in lines:\n",
    "        terms = line.split('\\t')\n",
    "        items = []\n",
    "        for i in terms[1:]:\n",
    "            if test_data:\n",
    "                ent, score = i, None\n",
    "\n",
    "            else:\n",
    "                colon_separated = i.split(':')\n",
    "                ent = ':'.join(colon_separated[:-1])\n",
    "                score = int(colon_separated[-1])\n",
    "            items.append((ent, score))\n",
    "        if len(items) == 0:\n",
    "            continue\n",
    "        parsing_result.append((terms[0], items))\n",
    "    return parsing_result\n",
    "\n",
    "\n",
    "# take celebrity as example\n",
    "cel_train_data = LoadInData(trainset_locs[0])\n",
    "\n",
    "\n",
    "def EvaluateByRank(strategy, seed=None, train_data=cel_train_data, title=\"\", export_report_at=None):\n",
    "    if seed is None:\n",
    "        seed = random.Random()\n",
    "    score_results = []\n",
    "    \n",
    "    report_data = {\"query_results\" : [], \"title\" : title}\n",
    "    \n",
    "    for q_id, (query, gs_result) in enumerate(train_data):\n",
    "        shuffled_result = copy.copy(gs_result)\n",
    "        seed.shuffle(shuffled_result)\n",
    "        my_result = strategy(query, [i for (i, t) in shuffled_result])\n",
    "        gs_result = [i for i, t in gs_result if t == 1]\n",
    "        \n",
    "        report_item = {\"term\" : query, \"ranked\" : [], \"id\" : q_id}\n",
    "        for r in my_result:\n",
    "            report_item['ranked'].append(\n",
    "                {'is_gs' : (r in gs_result), 'entity' : r})\n",
    "        map_score = apk(gs_result, my_result, len(shuffled_result))\n",
    "        report_item['MAP'] = map_score\n",
    "        report_data[\"query_results\"].append(report_item)\n",
    "        \n",
    "        score_results.append(map_score)\n",
    "    \n",
    "    map_value = sum(score_results) / len(score_results)\n",
    "    \n",
    "    report_data['map_value'] = map_value\n",
    "    if export_report_at is not None:\n",
    "        with open(export_report_at, 'w') as ofile:\n",
    "            html = environment.get_template('error_analysis.html').render(report_data)\n",
    "            ofile.write(html.encode('utf8'))\n",
    "    \n",
    "    return map_value\n",
    "\n",
    "def EvaluateAllByRank(strategy, seed=None):\n",
    "    if seed is None:\n",
    "        seed = random.Random()\n",
    "    result = {}\n",
    "    for sub_task, train_loc in zip(sub_tasks, trainset_locs):\n",
    "        train_data = LoadInData(train_loc)\n",
    "        result[sub_task] = EvaluateByRank(strategy, seed, train_data)\n",
    "    return result\n",
    "\n",
    "def OrderByScore(func):\n",
    "    def wrappee(q, results):\n",
    "        return [r for s, r in sorted([\n",
    "                    (func(q, r), r) for r in results\n",
    "                ], reverse=True)]\n",
    "    return wrappee\n",
    "\n",
    "def ExportResultsWithStrategy(strategy):\n",
    "    for output_filename, testdata_loc in zip(output_locs, devset_locs):\n",
    "        testdata = LoadInData(testdata_loc, test_data=True)\n",
    "        with open(output_filename, 'w') as ofile:\n",
    "            for query, entries in testdata:\n",
    "                my_result = strategy(query, [i for (i, t) in entries])\n",
    "                print >> ofile, '\\t'.join([query] + my_result).encode('gbk')\n",
    "                \n",
    "def BuildCutoffStrategy(strategy, cutoff):\n",
    "    def wrappee(*args, **kw):\n",
    "        result = strategy(*args, **kw)\n",
    "        return result[:cutoff]\n",
    "    return wrappee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I'll try the aforementioned feature sets, and throwing them into a ML framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baike_csv_loc = '../entities_db/baike.csv'\n",
    "entity_summary_map = dict()\n",
    "with open(baike_csv_loc) as infile:\n",
    "    for row in csv.DictReader(infile):\n",
    "        name = row['entity_name']\n",
    "        summary = row['summary']\n",
    "        entity_summary_map[name] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ExtractNCharOverlapFeature((q_type, query), entity):\n",
    "    return [(q_type + 'NCharOverlap', len(set(query).intersection(set(entity))))]\n",
    "\n",
    "def ExtractCharOverlapFeature((q_type, query), entity):\n",
    "    return [(q_type + 'CharOverlap=%s' % i, 1) for i in set(query).intersection(set(entity))]\n",
    "\n",
    "def ExtractNSumCharOverlapFeature((q_type, query), entity):\n",
    "    try:\n",
    "        summary = entity_summary_map[entity]\n",
    "        return [(q_type + 'NSumCharOverlap', len(set(query).intersection(set(summary))))]\n",
    "    except:\n",
    "        return [(q_type + 'NO_SUMMARY', 1)]\n",
    "\n",
    "def ExtractSumCharOverlapFeature((q_type, query), entity):\n",
    "    try:\n",
    "        summary = entity_summary_map[entity]\n",
    "        return [(q_type + 'SumCharOverlap=%s' % i, 1) for i in set(query).intersection(set(summary))]\n",
    "    except:\n",
    "        return [(q_type + 'NO_SUMMARY', 1)]\n",
    "    \n",
    "def CombinedModel(*models):\n",
    "    def wrappee(*args, **kw):\n",
    "        result = []\n",
    "        for model in models:\n",
    "            result.extend(model(*args, **kw))\n",
    "        return result\n",
    "    return wrappee\n",
    "\n",
    "ExtractAllFeatures = CombinedModel(ExtractNCharOverlapFeature,\n",
    "                                  ExtractCharOverlapFeature,\n",
    "                                  ExtractNSumCharOverlapFeature,\n",
    "                                  ExtractSumCharOverlapFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC 62.74%+-0.99%\n"
     ]
    }
   ],
   "source": [
    "all_train_data = []\n",
    "for q_type, trainset_loc in zip(sub_tasks, trainset_locs):\n",
    "    all_train_data.extend([(q_type, i) for i in LoadInData(trainset_loc)])\n",
    "\n",
    "ml_train_data = []\n",
    "for q_type, (query, entity_info_list) in all_train_data:\n",
    "    for ent, gs in entity_info_list:\n",
    "        ml_train_data.append((dict(ExtractAllFeatures((q_type, query), ent)), gs))\n",
    "\n",
    "v = feature_extraction.DictVectorizer()\n",
    "D = [d for d, y in ml_train_data]\n",
    "Y = np.array([int(y) for d, y in ml_train_data])\n",
    "X = v.fit_transform(D)\n",
    "logistic_regression = linear_model.LogisticRegression()\n",
    "scores = cross_validation.cross_val_score(logistic_regression, X, Y, cv=10, scoring='roc_auc')\n",
    "print \"AUC %.2f%%+-%.2f%%\" % (100 * np.mean(scores), 100 * np.std(scores))\n",
    "\n",
    "logreg_model = logistic_regression.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RankderByModelProb(model_type):\n",
    "    @OrderByScore\n",
    "    def ScorerByModel(q, r):\n",
    "        d = [dict(ExtractAllFeatures((model_type, q), r))]\n",
    "        x = v.transform(d)\n",
    "        result = logreg_model.predict_proba(x)\n",
    "        return result[0][1]\n",
    "    \n",
    "    return ScorerByModel\n",
    "\n",
    "def ExportResultsWithStrategy(strategy, model_type):\n",
    "    model_ind = sub_tasks.index(model_type)\n",
    "    output_filename, testdata_loc = output_locs[model_ind], devset_locs[model_ind]\n",
    "    testdata = LoadInData(testdata_loc, test_data=True)\n",
    "    with open(output_filename, 'w') as ofile:\n",
    "        for query, entries in testdata:\n",
    "            my_result = strategy(query, [i for (i, t) in entries])\n",
    "            print >> ofile, '\\t'.join([query] + my_result).encode('gbk')\n",
    "            \n",
    "for t in sub_tasks:\n",
    "    ExportResultsWithStrategy(RankderByModelProb(t), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ExportResultsWithStrategy(BuildCutoffStrategy(RankderByModelProb('restaurant'), 70), 'restaurant')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
